{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Luv4as/ia4good-rede-neural/blob/master/gnn-recommender-system.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c94c09df",
      "metadata": {
        "id": "c94c09df"
      },
      "source": [
        "# üöÄ Sistema de Recomenda√ß√£o com Graph Neural Networks (GNN)\n",
        "\n",
        "## üìã Objetivo do Experimento\n",
        "\n",
        "Este notebook implementa e compara diferentes arquiteturas de GNN para sistemas de recomenda√ß√£o:\n",
        "\n",
        "1. **NGCF** (Neural Graph Collaborative Filtering) - Baseline\n",
        "2. **LightGCN** - Vers√£o simplificada do NGCF\n",
        "3. **UltraGCN** - Estado da arte (SOTA)\n",
        "\n",
        "## üéØ Plano de Experimentos\n",
        "\n",
        "1. **Experimento 1**: NGCF vs LightGCN no dataset Yelp\n",
        "2. **Experimento 2**: NGCF vs LightGCN no dataset Amazon Books\n",
        "3. **Experimento 3**: UltraGCN em ambos os datasets\n",
        "4. **An√°lise Final**: Compara√ß√£o completa dos resultados\n",
        "\n",
        "## ‚öôÔ∏è Configura√ß√£o do Ambiente\n",
        "\n",
        "**üìñ Veja o arquivo `README.md` para instru√ß√µes completas de instala√ß√£o!**\n",
        "\n",
        "- **Conda**: `conda env create -f environment.yml`\n",
        "- **Pip**: `pip install -r requirements.txt`\n",
        "- **Kaggle**: Upload direto (plug & play)\n",
        "\n",
        "## üîß Configura√ß√µes Otimizadas\n",
        "\n",
        "- Datasets reduzidos com k-core filtering\n",
        "- Hiperpar√¢metros ajustados para execu√ß√£o r√°pida\n",
        "- Suporte autom√°tico para GPU quando dispon√≠vel\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bfba50f3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfba50f3",
        "outputId": "fd05d20d-20e7-45d5-ac67-6754a40f9984"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì torch-geometric instalado com sucesso\n"
          ]
        }
      ],
      "source": [
        "# Instala√ß√£o das depend√™ncias (otimizada para Kaggle)\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "\n",
        "def install_package(package):\n",
        "    try:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package, \"-q\"])\n",
        "        print(f\"‚úì {package} instalado com sucesso\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚úó Erro ao instalar {package}: {e}\")\n",
        "\n",
        "\n",
        "# Tentar instalar PyTorch Geometric com fallback\n",
        "try:\n",
        "    install_package(\"torch-geometric\")\n",
        "    install_package(\"torch-scatter\")\n",
        "    install_package(\"torch-sparse\")\n",
        "except Exception:\n",
        "    print(\"Usando instala√ß√£o alternativa...\")\n",
        "    install_package(\"torch-geometric==2.3.1\")\n",
        "\n",
        "print(\"Instala√ß√£o conclu√≠da!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84fceed5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84fceed5",
        "outputId": "aa3fa032-72d3-4b7a-95d8-9972875f3dc7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Executando no Kaggle: False\n",
            "CUDA dispon√≠vel: False\n"
          ]
        }
      ],
      "source": [
        "# Valida√ß√µes iniciais e configura√ß√µes para Kaggle\n",
        "import warnings\n",
        "import os\n",
        "import torch\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Verificar se estamos no Kaggle\n",
        "is_kaggle = \"KAGGLE_WORKING_DIR\" in os.environ\n",
        "print(f\"Executando no Kaggle: {is_kaggle}\")\n",
        "\n",
        "# Configura√ß√µes de mem√≥ria para Kaggle\n",
        "if is_kaggle:\n",
        "    import gc\n",
        "\n",
        "    gc.collect()\n",
        "    print(\"Configura√ß√µes de mem√≥ria otimizadas para Kaggle\")\n",
        "\n",
        "print(f\"CUDA dispon√≠vel: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(\n",
        "        f\"Mem√≥ria GPU: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c32d001b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c32d001b",
        "outputId": "06cf7005-0a75-46a4-a5a8-2b654e25fefe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usando dispositivo: cpu\n",
            "Vers√£o do PyTorch Geometric: 2.6.1\n"
          ]
        }
      ],
      "source": [
        "# Importa√ß√µes principais\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "import random\n",
        "\n",
        "# PyTorch Geometric para os datasets\n",
        "import torch_geometric\n",
        "from torch_geometric.datasets import Yelp, AmazonBook\n",
        "\n",
        "# Para construir a matriz de adjac√™ncia\n",
        "from scipy.sparse import coo_matrix, hstack, vstack\n",
        "\n",
        "import time\n",
        "import gc\n",
        "\n",
        "\n",
        "# --- Configura√ß√µes Globais e Reprodutibilidade ---\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "set_seed(42)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Usando dispositivo: {device}\")\n",
        "print(f\"Vers√£o do PyTorch Geometric: {torch_geometric.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ef1e504",
      "metadata": {
        "id": "5ef1e504"
      },
      "source": [
        "# Prepara√ß√£o dos Dados\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f241bce1",
      "metadata": {
        "id": "f241bce1"
      },
      "outputs": [],
      "source": [
        "# Converter os dados do PyG para DataFrames pandas\n",
        "def pyg_to_dataframe(data):\n",
        "    # Check if the data is HeteroData\n",
        "    if isinstance(data, torch_geometric.data.HeteroData):\n",
        "        # Assuming the user-item interaction edges are stored under ('user', 'rates', 'item')\n",
        "        # This might need adjustment based on the specific dataset structure if different\n",
        "        try:\n",
        "            edge_index = data['user', 'rates', 'item'].edge_index.numpy()\n",
        "            # For heterogeneous data, ratings might be stored differently or not at all.\n",
        "            # Assuming implicit feedback (rating = 1) for now if not explicitly available.\n",
        "            if hasattr(data['user', 'rates', 'item'], 'y') and data['user', 'rates', 'item'].y is not None:\n",
        "                 ratings = data['user', 'rates', 'item'].y.numpy()\n",
        "                 if ratings.ndim != 1:\n",
        "                     ratings = ratings.flatten()\n",
        "            else:\n",
        "                 ratings = np.ones(edge_index.shape[1]) # Use shape[1] for number of edges\n",
        "        except KeyError:\n",
        "            # Handle cases where the edge type might be different\n",
        "            print(\"Warning: Assuming edge type ('user', 'rates', 'item') not found. Trying other edge types.\")\n",
        "            # Attempt to find a user-item edge type - this is a heuristic and might need dataset-specific logic\n",
        "            user_item_edge_type = None\n",
        "            for edge_type in data.edge_types:\n",
        "                 if 'user' in edge_type and 'item' in edge_type:\n",
        "                      user_item_edge_type = edge_type\n",
        "                      break\n",
        "\n",
        "            if user_item_edge_type:\n",
        "                 print(f\"Using edge type: {user_item_edge_type}\")\n",
        "                 edge_index = data[user_item_edge_type].edge_index.numpy()\n",
        "                 if hasattr(data[user_item_edge_type], 'y') and data[user_item_edge_type].y is not None:\n",
        "                      ratings = data[user_item_edge_type].y.numpy()\n",
        "                      if ratings.ndim != 1:\n",
        "                          ratings = ratings.flatten()\n",
        "                 else:\n",
        "                      ratings = np.ones(edge_index.shape[1])\n",
        "            else:\n",
        "                 raise ValueError(\"Could not find a suitable user-item edge type in HeteroData.\")\n",
        "\n",
        "\n",
        "        user_ids = edge_index[0]\n",
        "        item_ids = edge_index[1]\n",
        "\n",
        "        # Ensure user_ids and item_ids are 1D (should be the case for edge_index)\n",
        "        if user_ids.ndim != 1:\n",
        "            user_ids = user_ids.flatten()\n",
        "        if item_ids.ndim != 1:\n",
        "            item_ids = item_ids.flatten()\n",
        "\n",
        "        num_interactions = len(user_ids)\n",
        "\n",
        "        # Ensure ratings has the same length as user_ids/item_ids\n",
        "        if len(ratings) > num_interactions:\n",
        "            ratings = ratings[:num_interactions]\n",
        "        elif len(ratings) < num_interactions:\n",
        "             print(f\"Warning: Ratings array shorter than edge index ({len(ratings)} vs {num_interactions}). Padding with 1s.\")\n",
        "             ratings = np.concatenate([ratings, np.ones(num_interactions - len(ratings))])\n",
        "\n",
        "\n",
        "    else: # Handle homogeneous Data\n",
        "        edge_index = data.edge_index.numpy()\n",
        "        user_ids = edge_index[0]\n",
        "        item_ids = edge_index[1]\n",
        "\n",
        "        # Ensure user_ids and item_ids are 1D\n",
        "        if user_ids.ndim != 1:\n",
        "            user_ids = user_ids.flatten()\n",
        "        if item_ids.ndim != 1:\n",
        "            item_ids = item_ids.flatten()\n",
        "\n",
        "        num_interactions = len(user_ids)\n",
        "\n",
        "        # Handle data.y potentially not existing or having a different shape\n",
        "        if hasattr(data, 'y') and data.y is not None:\n",
        "            ratings = data.y.numpy()\n",
        "            if ratings.ndim != 1:\n",
        "                ratings = ratings.flatten()\n",
        "            # Ensure ratings has the same length as user_ids/item_ids\n",
        "            if len(ratings) > num_interactions:\n",
        "                ratings = ratings[:num_interactions]\n",
        "            elif len(ratings) < num_interactions:\n",
        "                 print(f\"Warning: Ratings array shorter than edge index ({len(ratings)} vs {num_interactions}). Padding with 1s.\")\n",
        "                 ratings = np.concatenate([ratings, np.ones(num_interactions - len(ratings))])\n",
        "        else:\n",
        "            # Assume implicit feedback if no ratings are provided\n",
        "            ratings = np.ones(num_interactions)\n",
        "\n",
        "    # Final check on lengths before creating DataFrame\n",
        "    if len(user_ids) != len(item_ids) or len(user_ids) != len(ratings):\n",
        "         raise ValueError(f\"Length mismatch: user_ids={len(user_ids)}, item_ids={len(item_ids)}, ratings={len(ratings)}\")\n",
        "\n",
        "\n",
        "    df = pd.DataFrame({\"user_id\": user_ids, \"item_id\": item_ids, \"rating\": ratings})\n",
        "    return df\n",
        "\n",
        "\n",
        "def filter_k_core(df, k=20):\n",
        "    print(f\"\\nIniciando filtragem k-core com k={k}...\")\n",
        "    while True:\n",
        "        # Contar intera√ß√µes por usu√°rio e item\n",
        "        user_counts = df[\"user_id\"].value_counts()\n",
        "        item_counts = df[\"item_id\"].value_counts()\n",
        "\n",
        "        # Encontrar usu√°rios e itens com menos de K intera√ß√µes\n",
        "        weak_users = user_counts[user_counts < k].index\n",
        "        weak_items = item_counts[item_counts < k].index\n",
        "\n",
        "        # Se n√£o h√° mais ningu√©m para remover, o processo terminou\n",
        "        if len(weak_users) == 0 and len(weak_items) == 0:\n",
        "            print(\"Filtragem k-core conclu√≠da.\")\n",
        "            break\n",
        "\n",
        "        # Remover as linhas com usu√°rios ou itens fracos\n",
        "        print(\n",
        "            f\"Removendo {len(weak_users)} usu√°rios e {len(weak_items)} itens fracos...\"\n",
        "        )\n",
        "        df = df[~df[\"user_id\"].isin(weak_users)]\n",
        "        df = df[~df[\"item_id\"].isin(weak_items)]\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "# Reindexar IDs de usu√°rios e itens para come√ßarem em 0 e serem cont√≠nuos\n",
        "def reindex_ids(df):\n",
        "    print(\"\\nReindexando IDs de usu√°rios e itens...\")\n",
        "    user_id_mapping = {\n",
        "        old_id: new_id for new_id, old_id in enumerate(df[\"user_id\"].unique())\n",
        "    }\n",
        "    item_id_mapping = {\n",
        "        old_id: new_id for new_id, old_id in enumerate(df[\"item_id\"].unique())\n",
        "    }\n",
        "\n",
        "    df[\"user_id\"] = df[\"user_id\"].map(user_id_mapping)\n",
        "    df[\"item_id\"] = df[\"item_id\"].map(item_id_mapping)\n",
        "\n",
        "    num_users = len(user_id_mapping)\n",
        "    num_items = len(item_id_mapping)\n",
        "    print(f\"N√∫mero de usu√°rios: {num_users}, N√∫mero de itens: {num_items}\")\n",
        "    return df, num_users, num_items\n",
        "\n",
        "\n",
        "# Dividir os dados em treino e teste\n",
        "def train_test_split(df, test_size=0.2):\n",
        "    print(\"\\nDividindo os dados em treino e teste...\")\n",
        "    df = df.sample(frac=1, random_state=42).reset_index(\n",
        "        drop=True\n",
        "    )  # Embaralhar os dados\n",
        "    test_indices = []\n",
        "    train_indices = []\n",
        "\n",
        "    user_group = df.groupby(\"user_id\")\n",
        "    for user_id, group in user_group:\n",
        "        n_interactions = len(group)\n",
        "        n_test = max(\n",
        "            1, int(n_interactions * test_size)\n",
        "        )  # Garantir pelo menos uma intera√ß√£o no teste\n",
        "        test_indices.extend(group.index[:n_test])\n",
        "        train_indices.extend(group.index[n_test:])\n",
        "\n",
        "    test_df = df.loc[test_indices].reset_index(drop=True)\n",
        "    train_df = df.loc[train_indices].reset_index(drop=True)\n",
        "\n",
        "    print(\n",
        "        f\"N√∫mero de intera√ß√µes de treino: {len(train_df)}, N√∫mero de intera√ß√µes de teste: {len(test_df)}\"\n",
        "    )\n",
        "    return train_df, test_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1890d777",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "1890d777",
        "outputId": "109c8bc5-0ba6-4925-eec4-bf2197b1a685"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== CARREGANDO DATASET YELP ===\n",
            "Dataset Yelp carregado.\n",
            "\n",
            "=== CARREGANDO DATASET AMAZON BOOKS ===\n",
            "Dataset Amazon Books carregado.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'EdgeStorage' object has no attribute 'edge_index'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2578994179.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m# Transformar os dados do PyG em DataFrames pandas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0myelp_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyg_to_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myelp_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mamazon_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyg_to_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamazon_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nTransforma√ß√£o dos dados conclu√≠da.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1073835536.py\u001b[0m in \u001b[0;36mpyg_to_dataframe\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;31m# This might need adjustment based on the specific dataset structure if different\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0medge_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'user'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rates'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'item'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m             \u001b[0;31m# For heterogeneous data, ratings might be stored differently or not at all.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0;31m# Assuming implicit feedback (rating = 1) for now if not explicitly available.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch_geometric/data/storage.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             raise AttributeError(\n\u001b[0m\u001b[1;32m     97\u001b[0m                 \u001b[0;34mf\"'{self.__class__.__name__}' object has no attribute '{key}'\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             ) from None\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'EdgeStorage' object has no attribute 'edge_index'"
          ]
        }
      ],
      "source": [
        "# Fun√ß√£o para reduzir dataset (para execu√ß√£o mais r√°pida)\n",
        "def reduce_dataset(df, max_users=5000, max_items=3000):\n",
        "    \"\"\"Reduz o dataset mantendo apenas os usu√°rios e itens mais ativos\"\"\"\n",
        "    print(f\"Dataset original: {len(df)} intera√ß√µes\")\n",
        "\n",
        "    # Pegar os usu√°rios mais ativos\n",
        "    user_counts = df[\"user_id\"].value_counts()\n",
        "    top_users = user_counts.head(max_users).index\n",
        "\n",
        "    # Filtrar por usu√°rios mais ativos\n",
        "    df_reduced = df[df[\"user_id\"].isin(top_users)]\n",
        "\n",
        "    # Pegar os itens mais populares\n",
        "    item_counts = df_reduced[\"item_id\"].value_counts()\n",
        "    top_items = item_counts.head(max_items).index\n",
        "\n",
        "    # Filtrar por itens mais populares\n",
        "    df_reduced = df_reduced[df_reduced[\"item_id\"].isin(top_items)]\n",
        "\n",
        "    print(f\"Dataset reduzido: {len(df_reduced)} intera√ß√µes\")\n",
        "    return df_reduced\n",
        "\n",
        "\n",
        "# Carregar o dataset Yelp do PyG\n",
        "print(\"\\n=== CARREGANDO DATASET YELP ===\")\n",
        "yelp_dataset = Yelp(root=\"data/Yelp\")\n",
        "yelp_data = yelp_dataset[0]\n",
        "print(\"Dataset Yelp carregado.\")\n",
        "\n",
        "# Carregar o dataset Amazon do PyG\n",
        "print(\"\\n=== CARREGANDO DATASET AMAZON BOOKS ===\")\n",
        "amazon_dataset = AmazonBook(root=\"data/AmazonBook\")\n",
        "amazon_data = amazon_dataset[0]\n",
        "print(\"Dataset Amazon Books carregado.\")\n",
        "\n",
        "# Transformar os dados do PyG em DataFrames pandas\n",
        "yelp_df = pyg_to_dataframe(yelp_data)\n",
        "amazon_df = pyg_to_dataframe(amazon_data)\n",
        "print(\"\\nTransforma√ß√£o dos dados conclu√≠da.\")\n",
        "\n",
        "# Reduzir datasets para execu√ß√£o mais r√°pida\n",
        "print(\"\\n=== REDUZINDO DATASETS ===\")\n",
        "yelp_df = reduce_dataset(yelp_df, max_users=3000, max_items=2000)\n",
        "amazon_df = reduce_dataset(amazon_df, max_users=3000, max_items=2000)\n",
        "\n",
        "# Aplicar filtragem k-core (com k menor para datasets reduzidos)\n",
        "print(\"\\n=== APLICANDO FILTRAGEM K-CORE ===\")\n",
        "yelp_df = filter_k_core(yelp_df, k=5)  # k menor para datasets reduzidos\n",
        "amazon_df = filter_k_core(amazon_df, k=5)\n",
        "\n",
        "# Reindexar IDs de usu√°rios e itens\n",
        "print(\"\\n=== REINDEXANDO IDs ===\")\n",
        "yelp_df, num_yelp_users, num_yelp_items = reindex_ids(yelp_df)\n",
        "amazon_df, num_amazon_users, num_amazon_items = reindex_ids(amazon_df)\n",
        "\n",
        "# Dividir os dados em treino e teste\n",
        "print(\"\\n=== DIVIDINDO TREINO/TESTE ===\")\n",
        "yelp_train_df, yelp_test_df = train_test_split(yelp_df, test_size=0.2)\n",
        "amazon_train_df, amazon_test_df = train_test_split(amazon_df, test_size=0.2)\n",
        "\n",
        "print(\"\\n=== ESTAT√çSTICAS FINAIS ===\")\n",
        "print(f\"Yelp - Usu√°rios: {num_yelp_users}, Itens: {num_yelp_items}\")\n",
        "print(f\"Amazon - Usu√°rios: {num_amazon_users}, Itens: {num_amazon_items}\")\n",
        "print(\"Pr√©-processamento conclu√≠do!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "940ccaca",
      "metadata": {
        "id": "940ccaca"
      },
      "source": [
        "# Constru√ß√£o do Grafo e Matriz de Adjac√™ncia\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e46a8637",
      "metadata": {
        "id": "e46a8637"
      },
      "outputs": [],
      "source": [
        "def get_adj_matrix(train_df, num_users, num_items):\n",
        "    # Validar dados de entrada\n",
        "    if len(train_df) == 0:\n",
        "        raise ValueError(\"DataFrame de treino est√° vazio!\")\n",
        "    if num_users <= 0 or num_items <= 0:\n",
        "        raise ValueError(\n",
        "            f\"N√∫mero inv√°lido de usu√°rios ({num_users}) ou itens ({num_items})\"\n",
        "        )\n",
        "\n",
        "    # Criar uma matriz de adjac√™ncia esparsa (usu√°rios x itens)\n",
        "    adj_mat = coo_matrix(\n",
        "        (np.ones(len(train_df)), (train_df[\"user_id\"], train_df[\"item_id\"])),\n",
        "        shape=(num_users, num_items),\n",
        "    )\n",
        "\n",
        "    # Construir a matriz de adjac√™ncia completa (usu√°rios+itens x usu√°rios+itens)\n",
        "    # R = [[0, A], [A.T, 0]]\n",
        "    A = adj_mat\n",
        "    A_t = A.transpose()\n",
        "\n",
        "    # Criar blocos da matriz\n",
        "    upper_left = coo_matrix((num_users, num_users))  # Zero block\n",
        "    upper_right = A  # User-Item connections\n",
        "    lower_left = A_t  # Item-User connections\n",
        "    lower_right = coo_matrix((num_items, num_items))  # Zero block\n",
        "\n",
        "    upper_block = hstack([upper_left, upper_right])\n",
        "    lower_block = hstack([lower_left, lower_right])\n",
        "    adj_full = vstack([upper_block, lower_block])\n",
        "\n",
        "    # Converter para COO format para normaliza√ß√£o\n",
        "    adj_full = adj_full.tocoo()\n",
        "\n",
        "    # Normaliza√ß√£o da matriz de adjac√™ncia (D^-0.5 * A * D^-0.5)\n",
        "    degree = np.array(adj_full.sum(axis=1)).flatten()\n",
        "    degree[degree == 0] = 1  # Evitar divis√£o por zero\n",
        "\n",
        "    # Criar matriz diagonal de graus\n",
        "    d_inv_sqrt = np.power(degree, -0.5)\n",
        "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.0\n",
        "\n",
        "    # Aplicar normaliza√ß√£o\n",
        "    adj_full.data = adj_full.data * d_inv_sqrt[adj_full.row] * d_inv_sqrt[adj_full.col]\n",
        "\n",
        "    # Converter para tensor esparso do PyTorch\n",
        "    indices = torch.LongTensor(np.vstack((adj_full.row, adj_full.col)))\n",
        "    values = torch.FloatTensor(adj_full.data)\n",
        "    shape = torch.Size(adj_full.shape)\n",
        "\n",
        "    adj_norm_sparse = torch.sparse_coo_tensor(indices, values, shape).to(device)\n",
        "\n",
        "    return adj_norm_sparse\n",
        "\n",
        "\n",
        "# Criar matrizes de adjac√™ncia para ambos os datasets\n",
        "print(\"=== CRIANDO MATRIZES DE ADJAC√äNCIA ===\")\n",
        "yelp_adj_matrix = get_adj_matrix(yelp_train_df, num_yelp_users, num_yelp_items)\n",
        "amazon_adj_matrix = get_adj_matrix(amazon_train_df, num_amazon_users, num_amazon_items)\n",
        "print(\"Matrizes de adjac√™ncia criadas para ambos os datasets.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9e675aa",
      "metadata": {
        "id": "b9e675aa"
      },
      "source": [
        "# Defini√ß√£o das Arquiteturas GNN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c812f0f",
      "metadata": {
        "id": "3c812f0f"
      },
      "outputs": [],
      "source": [
        "class NGCF(nn.Module):\n",
        "    # Implementa√ß√£o simplificada do NGCF\n",
        "    def __init__(self, num_users, num_items, adj_matrix, embedding_dim=64, n_layers=2):\n",
        "        super().__init__()\n",
        "        self.num_users = num_users\n",
        "        self.num_items = num_items\n",
        "        self.adj_matrix = adj_matrix\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
        "        self.item_embedding = nn.Embedding(num_items, embedding_dim)\n",
        "\n",
        "        # Camadas de transforma√ß√£o (a parte \"complexa\" do NGCF)\n",
        "        self.W1 = nn.ModuleList(\n",
        "            [nn.Linear(embedding_dim, embedding_dim) for _ in range(n_layers)]\n",
        "        )\n",
        "        self.W2 = nn.ModuleList(\n",
        "            [nn.Linear(embedding_dim, embedding_dim) for _ in range(n_layers)]\n",
        "        )\n",
        "\n",
        "        # Inicializa√ß√£o dos embeddings\n",
        "        nn.init.xavier_uniform_(self.user_embedding.weight)\n",
        "        nn.init.xavier_uniform_(self.item_embedding.weight)\n",
        "\n",
        "    def forward(self, users, pos_items, neg_items):\n",
        "        # Propaga√ß√£o no grafo\n",
        "        embeddings = torch.cat(\n",
        "            [self.user_embedding.weight, self.item_embedding.weight], dim=0\n",
        "        )\n",
        "        all_layer_embeddings = [embeddings]\n",
        "\n",
        "        for i in range(self.n_layers):\n",
        "            # Propaga√ß√£o\n",
        "            propagated_embeddings = torch.sparse.mm(self.adj_matrix, embeddings)\n",
        "\n",
        "            # Transforma√ß√µes (a parte que o LightGCN remove)\n",
        "            sum_embeddings = self.W1[i](propagated_embeddings)\n",
        "            bi_embeddings = self.W2[i](embeddings * propagated_embeddings)  # Intera√ß√£o\n",
        "\n",
        "            embeddings = F.leaky_relu(sum_embeddings + bi_embeddings)\n",
        "            all_layer_embeddings.append(embeddings)\n",
        "\n",
        "        # Concatena√ß√£o das camadas\n",
        "        final_embeddings = torch.cat(all_layer_embeddings, dim=1)\n",
        "\n",
        "        # Obter embeddings finais para o batch\n",
        "        u_final = final_embeddings[users]\n",
        "        pos_i_final = final_embeddings[pos_items + self.num_users]\n",
        "        neg_i_final = final_embeddings[neg_items + self.num_users]\n",
        "\n",
        "        return u_final, pos_i_final, neg_i_final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ba1fe78",
      "metadata": {
        "id": "7ba1fe78"
      },
      "outputs": [],
      "source": [
        "class LightGCN(nn.Module):\n",
        "    # Sua Modifica√ß√£o: NGCF simplificado\n",
        "    def __init__(self, num_users, num_items, adj_matrix, embedding_dim=64, n_layers=2):\n",
        "        super().__init__()\n",
        "        self.num_users = num_users\n",
        "        self.num_items = num_items\n",
        "        self.adj_matrix = adj_matrix\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
        "        self.item_embedding = nn.Embedding(num_items, embedding_dim)\n",
        "\n",
        "        nn.init.xavier_uniform_(self.user_embedding.weight)\n",
        "        nn.init.xavier_uniform_(self.item_embedding.weight)\n",
        "\n",
        "    def forward(self, users, pos_items, neg_items):\n",
        "        embeddings = torch.cat(\n",
        "            [self.user_embedding.weight, self.item_embedding.weight], dim=0\n",
        "        )\n",
        "        all_layer_embeddings = [embeddings]\n",
        "\n",
        "        for _ in range(self.n_layers):\n",
        "            # Apenas a propaga√ß√£o, sem W1, W2 ou ativa√ß√µes\n",
        "            embeddings = torch.sparse.mm(self.adj_matrix, embeddings)\n",
        "            all_layer_embeddings.append(embeddings)\n",
        "\n",
        "        # M√©dia das camadas\n",
        "        final_embeddings = torch.mean(torch.stack(all_layer_embeddings, dim=0), dim=0)\n",
        "\n",
        "        u_final = final_embeddings[users]\n",
        "        pos_i_final = final_embeddings[pos_items + self.num_users]\n",
        "        neg_i_final = final_embeddings[neg_items + self.num_users]\n",
        "\n",
        "        return u_final, pos_i_final, neg_i_final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49443030",
      "metadata": {
        "id": "49443030"
      },
      "outputs": [],
      "source": [
        "class UltraGCN(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_users,\n",
        "        num_items,\n",
        "        adj_matrix,\n",
        "        embedding_dim=64,\n",
        "        lambda_reg=1e-4,\n",
        "        gamma=1e-4,\n",
        "        beta=0.5,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.num_users = num_users\n",
        "        self.num_items = num_items\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.lambda_reg = lambda_reg\n",
        "        self.gamma = gamma\n",
        "        self.beta = beta\n",
        "\n",
        "        # Embeddings iniciais\n",
        "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
        "        self.item_embedding = nn.Embedding(num_items, embedding_dim)\n",
        "\n",
        "        # Inicializa√ß√£o dos embeddings\n",
        "        nn.init.normal_(self.user_embedding.weight, std=0.1)\n",
        "        nn.init.normal_(self.item_embedding.weight, std=0.1)\n",
        "\n",
        "        # Pr√©-computar graus para efici√™ncia\n",
        "        self.register_buffer(\"user_degrees\", self._compute_user_degrees(adj_matrix))\n",
        "        self.register_buffer(\"item_degrees\", self._compute_item_degrees(adj_matrix))\n",
        "\n",
        "    def _compute_user_degrees(self, adj_matrix):\n",
        "        \"\"\"Calcula os graus dos usu√°rios a partir da matriz de adjac√™ncia\"\"\"\n",
        "        # adj_matrix tem formato (num_users + num_items, num_users + num_items)\n",
        "        # Pegar apenas a parte dos usu√°rios (primeiras num_users linhas)\n",
        "        adj_dense = adj_matrix.to_dense()\n",
        "        user_part = adj_dense[: self.num_users, self.num_users :]  # usu√°rios -> itens\n",
        "        degrees = user_part.sum(dim=1)\n",
        "        return degrees.float()\n",
        "\n",
        "    def _compute_item_degrees(self, adj_matrix):\n",
        "        \"\"\"Calcula os graus dos itens a partir da matriz de adjac√™ncia\"\"\"\n",
        "        adj_dense = adj_matrix.to_dense()\n",
        "        item_part = adj_dense[self.num_users :, : self.num_users]  # itens -> usu√°rios\n",
        "        degrees = item_part.sum(dim=1)\n",
        "        return degrees.float()\n",
        "\n",
        "    def forward(self, users, pos_items, neg_items):\n",
        "        \"\"\"Forward pass simples - apenas lookup de embeddings\"\"\"\n",
        "        u_embeds = self.user_embedding(users)\n",
        "        pos_i_embeds = self.item_embedding(pos_items)\n",
        "        neg_i_embeds = self.item_embedding(neg_items)\n",
        "        return u_embeds, pos_i_embeds, neg_i_embeds\n",
        "\n",
        "    def calculate_loss(self, users, pos_items, neg_items):\n",
        "        \"\"\"Calcula a loss completa do UltraGCN\"\"\"\n",
        "        u_embeds, pos_i_embeds, neg_i_embeds = self.forward(users, pos_items, neg_items)\n",
        "\n",
        "        # 1. BPR Loss b√°sica\n",
        "        pos_scores = (u_embeds * pos_i_embeds).sum(dim=1)\n",
        "        neg_scores = (u_embeds * neg_i_embeds).sum(dim=1)\n",
        "        bpr_loss = F.softplus(neg_scores - pos_scores).mean()\n",
        "\n",
        "        # 2. Constraint Loss baseada nos graus dos n√≥s\n",
        "        # Pesos baseados nos graus (quanto maior o grau, maior o peso)\n",
        "        user_deg_weights = torch.sqrt(self.user_degrees[users] + 1e-8)\n",
        "        pos_item_deg_weights = torch.sqrt(self.item_degrees[pos_items] + 1e-8)\n",
        "        neg_item_deg_weights = torch.sqrt(self.item_degrees[neg_items] + 1e-8)\n",
        "\n",
        "        # Normalizar pesos\n",
        "        user_deg_weights = user_deg_weights / (user_deg_weights.max() + 1e-8)\n",
        "        pos_item_deg_weights = pos_item_deg_weights / (\n",
        "            pos_item_deg_weights.max() + 1e-8\n",
        "        )\n",
        "        neg_item_deg_weights = neg_item_deg_weights / (\n",
        "            neg_item_deg_weights.max() + 1e-8\n",
        "        )\n",
        "\n",
        "        # Constraint loss com pesos baseados em graus\n",
        "        constraint_loss = self.lambda_reg * (\n",
        "            (user_deg_weights * u_embeds.norm(p=2, dim=1)).mean()\n",
        "            + (pos_item_deg_weights * pos_i_embeds.norm(p=2, dim=1)).mean()\n",
        "            + (neg_item_deg_weights * neg_i_embeds.norm(p=2, dim=1)).mean()\n",
        "        )\n",
        "\n",
        "        # 3. Regulariza√ß√£o L2 adicional\n",
        "        l2_reg = self.gamma * (\n",
        "            u_embeds.norm(p=2, dim=1).mean()\n",
        "            + pos_i_embeds.norm(p=2, dim=1).mean()\n",
        "            + neg_i_embeds.norm(p=2, dim=1).mean()\n",
        "        )\n",
        "\n",
        "        total_loss = bpr_loss + constraint_loss + l2_reg\n",
        "        return total_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9462229",
      "metadata": {
        "id": "a9462229"
      },
      "source": [
        "# Fun√ß√µes de Avalia√ß√£o e Treinamento\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e362f68",
      "metadata": {
        "id": "8e362f68"
      },
      "outputs": [],
      "source": [
        "# Fun√ß√£o para amostragem de negativos (corrigida)\n",
        "def sample_negatives(train_df, num_items, num_samples=1):\n",
        "    user_pos_items = train_df.groupby(\"user_id\")[\"item_id\"].apply(set).to_dict()\n",
        "    neg_items = {}\n",
        "\n",
        "    for user, pos_set in user_pos_items.items():\n",
        "        neg_samples = []\n",
        "        max_attempts = num_samples * len(pos_set) * 10  # Limite de tentativas\n",
        "        attempts = 0\n",
        "\n",
        "        while len(neg_samples) < num_samples * len(pos_set) and attempts < max_attempts:\n",
        "            neg_id = random.randint(0, num_items - 1)\n",
        "            if neg_id not in pos_set and neg_id not in neg_samples:\n",
        "                neg_samples.append(neg_id)\n",
        "            attempts += 1\n",
        "\n",
        "        # Se n√£o conseguir amostras suficientes, preencher com aleat√≥rios\n",
        "        while len(neg_samples) < num_samples * len(pos_set):\n",
        "            neg_id = random.randint(0, num_items - 1)\n",
        "            neg_samples.append(neg_id)\n",
        "\n",
        "        neg_items[user] = neg_samples\n",
        "    return neg_items\n",
        "\n",
        "\n",
        "# Fun√ß√µes de M√©trica simplificadas\n",
        "def calculate_metrics(test_items, topk_items, k=20):\n",
        "    \"\"\"Calcula Recall@k e NDCG@k para um usu√°rio\"\"\"\n",
        "    hits = len(test_items.intersection(topk_items))\n",
        "\n",
        "    if len(test_items) == 0:\n",
        "        return 0.0, 0.0\n",
        "\n",
        "    # Recall@k\n",
        "    recall = hits / len(test_items)\n",
        "\n",
        "    # NDCG@k simplificado\n",
        "    dcg = sum(\n",
        "        1.0 / np.log2(i + 2) for i, item in enumerate(topk_items) if item in test_items\n",
        "    )\n",
        "    idcg = sum(1.0 / np.log2(i + 2) for i in range(min(len(test_items), k)))\n",
        "    ndcg = dcg / idcg if idcg > 0 else 0.0\n",
        "\n",
        "    return recall, ndcg\n",
        "\n",
        "\n",
        "# Fun√ß√£o de avalia√ß√£o corrigida\n",
        "def evaluate_model(model, test_df, train_df, num_users, num_items, k=20):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Obter embeddings finais de usu√°rios e itens\n",
        "        if isinstance(model, UltraGCN):\n",
        "            # Para UltraGCN, usar embeddings diretamente\n",
        "            all_user_embeds = model.user_embedding.weight\n",
        "            all_item_embeds = model.item_embedding.weight\n",
        "        else:\n",
        "            # Para NGCF e LightGCN, computar embeddings finais\n",
        "            # Usar embeddings completos depois da propaga√ß√£o\n",
        "            embeddings = torch.cat(\n",
        "                [model.user_embedding.weight, model.item_embedding.weight], dim=0\n",
        "            )\n",
        "\n",
        "            if isinstance(model, NGCF):\n",
        "                # Para NGCF, aplicar as camadas de propaga√ß√£o\n",
        "                all_layer_embeddings = [embeddings]\n",
        "                for i in range(model.n_layers):\n",
        "                    propagated_embeddings = torch.sparse.mm(\n",
        "                        model.adj_matrix, embeddings\n",
        "                    )\n",
        "                    sum_embeddings = model.W1[i](propagated_embeddings)\n",
        "                    bi_embeddings = model.W2[i](embeddings * propagated_embeddings)\n",
        "                    embeddings = F.leaky_relu(sum_embeddings + bi_embeddings)\n",
        "                    all_layer_embeddings.append(embeddings)\n",
        "                final_embeddings = torch.cat(all_layer_embeddings, dim=1)\n",
        "            else:\n",
        "                # Para LightGCN, aplicar propaga√ß√£o simples\n",
        "                all_layer_embeddings = [embeddings]\n",
        "                for _ in range(model.n_layers):\n",
        "                    embeddings = torch.sparse.mm(model.adj_matrix, embeddings)\n",
        "                    all_layer_embeddings.append(embeddings)\n",
        "                final_embeddings = torch.mean(\n",
        "                    torch.stack(all_layer_embeddings, dim=0), dim=0\n",
        "                )\n",
        "\n",
        "            all_user_embeds = final_embeddings[:num_users]\n",
        "            all_item_embeds = final_embeddings[num_users:]\n",
        "\n",
        "    # Preparar dados para avalia√ß√£o\n",
        "    user_pos_items = train_df.groupby(\"user_id\")[\"item_id\"].apply(set).to_dict()\n",
        "    test_user_pos_items = test_df.groupby(\"user_id\")[\"item_id\"].apply(set).to_dict()\n",
        "\n",
        "    total_recall = 0\n",
        "    total_ndcg = 0\n",
        "    num_test_users = 0\n",
        "\n",
        "    for user_id, test_items in tqdm(test_user_pos_items.items(), desc=\"Avaliando\"):\n",
        "        if user_id not in user_pos_items or user_id >= num_users:\n",
        "            continue\n",
        "\n",
        "        user_embed = all_user_embeds[user_id]\n",
        "        scores = torch.matmul(user_embed, all_item_embeds.t())\n",
        "\n",
        "        # Remover itens j√° vistos no treino\n",
        "        train_items = user_pos_items[user_id]\n",
        "        for item in train_items:\n",
        "            if item < len(scores):\n",
        "                scores[item] = -float(\"inf\")\n",
        "\n",
        "        # Pegar top-k itens\n",
        "        _, topk_items = torch.topk(scores, min(k, len(scores)))\n",
        "        topk_items_set = set(topk_items.cpu().numpy())\n",
        "\n",
        "        if len(test_items) > 0:\n",
        "            recall, ndcg = calculate_metrics(test_items, topk_items_set, k)\n",
        "            total_recall += recall\n",
        "            total_ndcg += ndcg\n",
        "\n",
        "        num_test_users += 1\n",
        "\n",
        "    if num_test_users == 0:\n",
        "        return 0.0, 0.0\n",
        "\n",
        "    return total_recall / num_test_users, total_ndcg / num_test_users\n",
        "\n",
        "\n",
        "# BPR Loss\n",
        "def bpr_loss(u_embeds, pos_i_embeds, neg_i_embeds):\n",
        "    pos_scores = (u_embeds * pos_i_embeds).sum(dim=1)\n",
        "    neg_scores = (u_embeds * neg_i_embeds).sum(dim=1)\n",
        "    return F.softplus(neg_scores - pos_scores).mean()\n",
        "\n",
        "\n",
        "# Fun√ß√£o de treinamento gen√©rica com batching (com valida√ß√µes)\n",
        "def train_model(\n",
        "    model,\n",
        "    train_df,\n",
        "    num_users,\n",
        "    num_items,\n",
        "    epochs=10,\n",
        "    lr=0.001,\n",
        "    batch_size=2048,\n",
        "    model_name=\"Modelo\",\n",
        "):\n",
        "    print(f\"\\n=== TREINANDO {model_name.upper()} ===\")\n",
        "\n",
        "    # Valida√ß√µes\n",
        "    if len(train_df) == 0:\n",
        "        raise ValueError(\"DataFrame de treino est√° vazio!\")\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    # Preparar dados para BPR\n",
        "    train_users = train_df[\"user_id\"].values\n",
        "    train_pos_items = train_df[\"item_id\"].values\n",
        "\n",
        "    # Gerar amostras negativas\n",
        "    try:\n",
        "        neg_item_samples = sample_negatives(train_df, num_items, num_samples=1)\n",
        "        train_neg_items = np.array([neg_item_samples[u][0] for u in train_users])\n",
        "    except Exception as e:\n",
        "        print(f\"Erro na amostragem de negativos: {e}\")\n",
        "        raise\n",
        "\n",
        "    # Criar √≠ndices para batching\n",
        "    num_samples = len(train_users)\n",
        "    indices = np.arange(num_samples)\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        np.random.shuffle(indices)  # Embaralhar dados a cada √©poca\n",
        "\n",
        "        num_batches = (num_samples + batch_size - 1) // batch_size\n",
        "\n",
        "        for batch_idx in range(num_batches):\n",
        "            start_idx = batch_idx * batch_size\n",
        "            end_idx = min((batch_idx + 1) * batch_size, num_samples)\n",
        "            batch_indices = indices[start_idx:end_idx]\n",
        "\n",
        "            # Preparar batch\n",
        "            batch_users = torch.LongTensor(train_users[batch_indices]).to(device)\n",
        "            batch_pos_items = torch.LongTensor(train_pos_items[batch_indices]).to(\n",
        "                device\n",
        "            )\n",
        "            batch_neg_items = torch.LongTensor(train_neg_items[batch_indices]).to(\n",
        "                device\n",
        "            )\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            if hasattr(model, \"calculate_loss\"):  # UltraGCN\n",
        "                loss = model.calculate_loss(\n",
        "                    batch_users, batch_pos_items, batch_neg_items\n",
        "                )\n",
        "            else:  # NGCF e LightGCN\n",
        "                u_embeds, pos_i_embeds, neg_i_embeds = model(\n",
        "                    batch_users, batch_pos_items, batch_neg_items\n",
        "                )\n",
        "                loss = bpr_loss(u_embeds, pos_i_embeds, neg_i_embeds)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / num_batches\n",
        "        if (epoch + 1) % 2 == 0:\n",
        "            print(f\"{model_name} - √âpoca {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    training_time = time.time() - start_time\n",
        "    print(f\"{model_name} treinado em {training_time:.2f}s\")\n",
        "\n",
        "    return training_time"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75c7ce8b",
      "metadata": {
        "id": "75c7ce8b"
      },
      "source": [
        "# Execu√ß√£o do Experimento\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "019f5e2d",
      "metadata": {
        "id": "019f5e2d"
      },
      "outputs": [],
      "source": [
        "# Hiperpar√¢metros otimizados para Kaggle\n",
        "EPOCHS = 3  # Reduzido para execu√ß√£o mais r√°pida no Kaggle\n",
        "BATCH_SIZE = 1024  # Reduzido para evitar problemas de mem√≥ria\n",
        "EMBEDDING_DIM = 32  # Reduzido para acelerar treinamento\n",
        "LEARNING_RATE = 0.001\n",
        "N_LAYERS = 2  # Para NGCF e LightGCN\n",
        "\n",
        "print(\"=== CONFIGURA√á√ïES DO EXPERIMENTO ===\")\n",
        "print(f\"√âpocas: {EPOCHS}\")\n",
        "print(f\"Tamanho do batch: {BATCH_SIZE}\")\n",
        "print(f\"Dimens√£o dos embeddings: {EMBEDDING_DIM}\")\n",
        "print(f\"Taxa de aprendizado: {LEARNING_RATE}\")\n",
        "print(f\"N√∫mero de camadas GNN: {N_LAYERS}\")\n",
        "print(f\"Dispositivo: {device}\")\n",
        "print(\"\\n‚ö†Ô∏è  Configura√ß√µes otimizadas para execu√ß√£o r√°pida no Kaggle\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60ee5bdb",
      "metadata": {
        "id": "60ee5bdb"
      },
      "outputs": [],
      "source": [
        "# Verifica√ß√£o de integridade dos dados antes dos experimentos\n",
        "print(\"=== VERIFICA√á√ÉO DE INTEGRIDADE DOS DADOS ===\")\n",
        "\n",
        "\n",
        "def verify_data_integrity():\n",
        "    \"\"\"Verifica se os dados est√£o corretos para os experimentos\"\"\"\n",
        "    checks_passed = 0\n",
        "    total_checks = 6\n",
        "\n",
        "    # Check 1: DataFrames n√£o vazios\n",
        "    if len(yelp_train_df) > 0 and len(yelp_test_df) > 0:\n",
        "        print(\"‚úì Yelp: Dados de treino e teste n√£o est√£o vazios\")\n",
        "        checks_passed += 1\n",
        "    else:\n",
        "        print(\"‚úó Yelp: Dados vazios!\")\n",
        "\n",
        "    # Check 2: Amazon DataFrames n√£o vazios\n",
        "    if len(amazon_train_df) > 0 and len(amazon_test_df) > 0:\n",
        "        print(\"‚úì Amazon: Dados de treino e teste n√£o est√£o vazios\")\n",
        "        checks_passed += 1\n",
        "    else:\n",
        "        print(\"‚úó Amazon: Dados vazios!\")\n",
        "\n",
        "    # Check 3: IDs v√°lidos no Yelp\n",
        "    if (\n",
        "        yelp_train_df[\"user_id\"].max() < num_yelp_users\n",
        "        and yelp_train_df[\"item_id\"].max() < num_yelp_items\n",
        "    ):\n",
        "        print(\"‚úì Yelp: IDs de usu√°rios e itens est√£o dentro dos limites\")\n",
        "        checks_passed += 1\n",
        "    else:\n",
        "        print(\"‚úó Yelp: IDs fora dos limites!\")\n",
        "\n",
        "    # Check 4: IDs v√°lidos no Amazon\n",
        "    if (\n",
        "        amazon_train_df[\"user_id\"].max() < num_amazon_users\n",
        "        and amazon_train_df[\"item_id\"].max() < num_amazon_items\n",
        "    ):\n",
        "        print(\"‚úì Amazon: IDs de usu√°rios e itens est√£o dentro dos limites\")\n",
        "        checks_passed += 1\n",
        "    else:\n",
        "        print(\"‚úó Amazon: IDs fora dos limites!\")\n",
        "\n",
        "    # Check 5: Matrizes de adjac√™ncia criadas\n",
        "    if yelp_adj_matrix is not None and amazon_adj_matrix is not None:\n",
        "        print(\"‚úì Matrizes de adjac√™ncia criadas com sucesso\")\n",
        "        checks_passed += 1\n",
        "    else:\n",
        "        print(\"‚úó Erro na cria√ß√£o das matrizes de adjac√™ncia!\")\n",
        "\n",
        "    # Check 6: Device dispon√≠vel\n",
        "    if device.type in [\"cuda\", \"cpu\"]:\n",
        "        print(f\"‚úì Dispositivo {device} dispon√≠vel para computa√ß√£o\")\n",
        "        checks_passed += 1\n",
        "    else:\n",
        "        print(\"‚úó Erro na configura√ß√£o do dispositivo!\")\n",
        "\n",
        "    print(f\"\\nResumo: {checks_passed}/{total_checks} verifica√ß√µes passaram\")\n",
        "\n",
        "    if checks_passed == total_checks:\n",
        "        print(\"üéâ Todos os dados est√£o √≠ntegros! Pronto para experimentos.\")\n",
        "        return True\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è  Alguns problemas detectados. Verifique os dados antes de continuar.\")\n",
        "        return False\n",
        "\n",
        "\n",
        "# Executar verifica√ß√£o\n",
        "data_ok = verify_data_integrity()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f74ef9b1",
      "metadata": {
        "id": "f74ef9b1"
      },
      "outputs": [],
      "source": [
        "# Verificar se os dados est√£o OK antes de executar experimentos\n",
        "if not data_ok:\n",
        "    print(\"‚ùå EXPERIMENTOS CANCELADOS!\")\n",
        "    print(\"Por favor, verifique os problemas nos dados acima antes de continuar.\")\n",
        "    print(\"Poss√≠veis solu√ß√µes:\")\n",
        "    print(\"- Reinstalar depend√™ncias\")\n",
        "    print(\"- Verificar conex√£o com internet para download dos datasets\")\n",
        "    print(\"- Reiniciar o kernel do notebook\")\n",
        "else:\n",
        "    print(\"‚úÖ Dados verificados! Iniciando experimentos...\")\n",
        "    print(\"üöÄ Execute as pr√≥ximas c√©lulas para rodar os experimentos.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3cb208e5",
      "metadata": {
        "id": "3cb208e5"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# EXPERIMENTO 1: NGCF vs LightGCN no Dataset YELP\n",
        "# =============================================================================\n",
        "\n",
        "if not data_ok:\n",
        "    print(\"‚ùå Pulando experimento 1 devido a problemas nos dados!\")\n",
        "else:\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"EXPERIMENTO 1: NGCF vs LightGCN no Dataset YELP\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # --- Modelo 1: NGCF no Yelp ---\n",
        "    model_ngcf_yelp = NGCF(\n",
        "        num_yelp_users, num_yelp_items, yelp_adj_matrix, EMBEDDING_DIM, N_LAYERS\n",
        "    ).to(device)\n",
        "\n",
        "    time_ngcf_yelp = train_model(\n",
        "        model_ngcf_yelp,\n",
        "        yelp_train_df,\n",
        "        num_yelp_users,\n",
        "        num_yelp_items,\n",
        "        epochs=EPOCHS,\n",
        "        lr=LEARNING_RATE,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        model_name=\"NGCF (Yelp)\",\n",
        "    )\n",
        "\n",
        "    recall_ngcf_yelp, ndcg_ngcf_yelp = evaluate_model(\n",
        "        model_ngcf_yelp, yelp_test_df, yelp_train_df, num_yelp_users, num_yelp_items\n",
        "    )\n",
        "    print(\n",
        "        f\"NGCF (Yelp) - Recall@20: {recall_ngcf_yelp:.4f}, NDCG@20: {ndcg_ngcf_yelp:.4f}\"\n",
        "    )\n",
        "\n",
        "    # --- Modelo 2: LightGCN no Yelp ---\n",
        "    model_lightgcn_yelp = LightGCN(\n",
        "        num_yelp_users, num_yelp_items, yelp_adj_matrix, EMBEDDING_DIM, N_LAYERS\n",
        "    ).to(device)\n",
        "\n",
        "    time_lightgcn_yelp = train_model(\n",
        "        model_lightgcn_yelp,\n",
        "        yelp_train_df,\n",
        "        num_yelp_users,\n",
        "        num_yelp_items,\n",
        "        epochs=EPOCHS,\n",
        "        lr=LEARNING_RATE,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        model_name=\"LightGCN (Yelp)\",\n",
        "    )\n",
        "\n",
        "    recall_lightgcn_yelp, ndcg_lightgcn_yelp = evaluate_model(\n",
        "        model_lightgcn_yelp, yelp_test_df, yelp_train_df, num_yelp_users, num_yelp_items\n",
        "    )\n",
        "    print(\n",
        "        f\"LightGCN (Yelp) - Recall@20: {recall_lightgcn_yelp:.4f}, NDCG@20: {ndcg_lightgcn_yelp:.4f}\"\n",
        "    )\n",
        "\n",
        "    print(\"\\nResumo Yelp:\")\n",
        "    print(\n",
        "        f\"NGCF: Recall={recall_ngcf_yelp:.4f}, NDCG={ndcg_ngcf_yelp:.4f}, Tempo={time_ngcf_yelp:.2f}s\"\n",
        "    )\n",
        "    print(\n",
        "        f\"LightGCN: Recall={recall_lightgcn_yelp:.4f}, NDCG={ndcg_lightgcn_yelp:.4f}, Tempo={time_lightgcn_yelp:.2f}s\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff1b2be1",
      "metadata": {
        "id": "ff1b2be1"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# EXPERIMENTO 2: NGCF vs LightGCN no Dataset AMAZON\n",
        "# =============================================================================\n",
        "\n",
        "if not data_ok:\n",
        "    print(\"‚ùå Pulando experimento 2 devido a problemas nos dados!\")\n",
        "else:\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"EXPERIMENTO 2: NGCF vs LightGCN no Dataset AMAZON\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # --- Modelo 1: NGCF no Amazon ---\n",
        "    model_ngcf_amazon = NGCF(\n",
        "        num_amazon_users, num_amazon_items, amazon_adj_matrix, EMBEDDING_DIM, N_LAYERS\n",
        "    ).to(device)\n",
        "\n",
        "    time_ngcf_amazon = train_model(\n",
        "        model_ngcf_amazon,\n",
        "        amazon_train_df,\n",
        "        num_amazon_users,\n",
        "        num_amazon_items,\n",
        "        epochs=EPOCHS,\n",
        "        lr=LEARNING_RATE,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        model_name=\"NGCF (Amazon)\",\n",
        "    )\n",
        "\n",
        "    recall_ngcf_amazon, ndcg_ngcf_amazon = evaluate_model(\n",
        "        model_ngcf_amazon,\n",
        "        amazon_test_df,\n",
        "        amazon_train_df,\n",
        "        num_amazon_users,\n",
        "        num_amazon_items,\n",
        "    )\n",
        "    print(\n",
        "        f\"NGCF (Amazon) - Recall@20: {recall_ngcf_amazon:.4f}, NDCG@20: {ndcg_ngcf_amazon:.4f}\"\n",
        "    )\n",
        "\n",
        "    # --- Modelo 2: LightGCN no Amazon ---\n",
        "    model_lightgcn_amazon = LightGCN(\n",
        "        num_amazon_users, num_amazon_items, amazon_adj_matrix, EMBEDDING_DIM, N_LAYERS\n",
        "    ).to(device)\n",
        "\n",
        "    time_lightgcn_amazon = train_model(\n",
        "        model_lightgcn_amazon,\n",
        "        amazon_train_df,\n",
        "        num_amazon_users,\n",
        "        num_amazon_items,\n",
        "        epochs=EPOCHS,\n",
        "        lr=LEARNING_RATE,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        model_name=\"LightGCN (Amazon)\",\n",
        "    )\n",
        "\n",
        "    recall_lightgcn_amazon, ndcg_lightgcn_amazon = evaluate_model(\n",
        "        model_lightgcn_amazon,\n",
        "        amazon_test_df,\n",
        "        amazon_train_df,\n",
        "        num_amazon_users,\n",
        "        num_amazon_items,\n",
        "    )\n",
        "    print(\n",
        "        f\"LightGCN (Amazon) - Recall@20: {recall_lightgcn_amazon:.4f}, NDCG@20: {ndcg_lightgcn_amazon:.4f}\"\n",
        "    )\n",
        "\n",
        "    print(\"\\nResumo Amazon:\")\n",
        "    print(\n",
        "        f\"NGCF: Recall={recall_ngcf_amazon:.4f}, NDCG={ndcg_ngcf_amazon:.4f}, Tempo={time_ngcf_amazon:.2f}s\"\n",
        "    )\n",
        "    print(\n",
        "        f\"LightGCN: Recall={recall_lightgcn_amazon:.4f}, NDCG={ndcg_lightgcn_amazon:.4f}, Tempo={time_lightgcn_amazon:.2f}s\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2cb7b8b3",
      "metadata": {
        "id": "2cb7b8b3"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# EXPERIMENTO 3: UltraGCN nos Datasets YELP e AMAZON\n",
        "# =============================================================================\n",
        "\n",
        "if not data_ok:\n",
        "    print(\"‚ùå Pulando experimento 3 devido a problemas nos dados!\")\n",
        "else:\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"EXPERIMENTO 3: UltraGCN nos Datasets YELP e AMAZON\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # --- UltraGCN no Yelp ---\n",
        "    model_ultragcn_yelp = UltraGCN(\n",
        "        num_yelp_users, num_yelp_items, yelp_adj_matrix, EMBEDDING_DIM\n",
        "    ).to(device)\n",
        "\n",
        "    time_ultragcn_yelp = train_model(\n",
        "        model_ultragcn_yelp,\n",
        "        yelp_train_df,\n",
        "        num_yelp_users,\n",
        "        num_yelp_items,\n",
        "        epochs=EPOCHS,\n",
        "        lr=LEARNING_RATE,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        model_name=\"UltraGCN (Yelp)\",\n",
        "    )\n",
        "\n",
        "    recall_ultragcn_yelp, ndcg_ultragcn_yelp = evaluate_model(\n",
        "        model_ultragcn_yelp, yelp_test_df, yelp_train_df, num_yelp_users, num_yelp_items\n",
        "    )\n",
        "    print(\n",
        "        f\"UltraGCN (Yelp) - Recall@20: {recall_ultragcn_yelp:.4f}, NDCG@20: {ndcg_ultragcn_yelp:.4f}\"\n",
        "    )\n",
        "\n",
        "    # --- UltraGCN no Amazon ---\n",
        "    model_ultragcn_amazon = UltraGCN(\n",
        "        num_amazon_users, num_amazon_items, amazon_adj_matrix, EMBEDDING_DIM\n",
        "    ).to(device)\n",
        "\n",
        "    time_ultragcn_amazon = train_model(\n",
        "        model_ultragcn_amazon,\n",
        "        amazon_train_df,\n",
        "        num_amazon_users,\n",
        "        num_amazon_items,\n",
        "        epochs=EPOCHS,\n",
        "        lr=LEARNING_RATE,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        model_name=\"UltraGCN (Amazon)\",\n",
        "    )\n",
        "\n",
        "    recall_ultragcn_amazon, ndcg_ultragcn_amazon = evaluate_model(\n",
        "        model_ultragcn_amazon,\n",
        "        amazon_test_df,\n",
        "        amazon_train_df,\n",
        "        num_amazon_users,\n",
        "        num_amazon_items,\n",
        "    )\n",
        "    print(\n",
        "        f\"UltraGCN (Amazon) - Recall@20: {recall_ultragcn_amazon:.4f}, NDCG@20: {ndcg_ultragcn_amazon:.4f}\"\n",
        "    )\n",
        "\n",
        "    print(\"\\nResumo UltraGCN:\")\n",
        "    print(\n",
        "        f\"Yelp: Recall={recall_ultragcn_yelp:.4f}, NDCG={ndcg_ultragcn_yelp:.4f}, Tempo={time_ultragcn_yelp:.2f}s\"\n",
        "    )\n",
        "    print(\n",
        "        f\"Amazon: Recall={recall_ultragcn_amazon:.4f}, NDCG={ndcg_ultragcn_amazon:.4f}, Tempo={time_ultragcn_amazon:.2f}s\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7abb51b9",
      "metadata": {
        "id": "7abb51b9"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# RESUMO FINAL: COMPARA√á√ÉO DE TODOS OS RESULTADOS\n",
        "# =============================================================================\n",
        "\n",
        "if not data_ok:\n",
        "    print(\"‚ùå N√£o √© poss√≠vel gerar resumo devido a problemas nos dados!\")\n",
        "else:\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"RESUMO FINAL - COMPARA√á√ÉO COMPLETA\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Criar tabela de resultados completa\n",
        "    results_data = {\n",
        "        \"Modelo\": [\"NGCF\", \"LightGCN\", \"NGCF\", \"LightGCN\", \"UltraGCN\", \"UltraGCN\"],\n",
        "        \"Dataset\": [\"Yelp\", \"Yelp\", \"Amazon\", \"Amazon\", \"Yelp\", \"Amazon\"],\n",
        "        \"Recall@20\": [\n",
        "            recall_ngcf_yelp,\n",
        "            recall_lightgcn_yelp,\n",
        "            recall_ngcf_amazon,\n",
        "            recall_lightgcn_amazon,\n",
        "            recall_ultragcn_yelp,\n",
        "            recall_ultragcn_amazon,\n",
        "        ],\n",
        "        \"NDCG@20\": [\n",
        "            ndcg_ngcf_yelp,\n",
        "            ndcg_lightgcn_yelp,\n",
        "            ndcg_ngcf_amazon,\n",
        "            ndcg_lightgcn_amazon,\n",
        "            ndcg_ultragcn_yelp,\n",
        "            ndcg_ultragcn_amazon,\n",
        "        ],\n",
        "        \"Tempo (s)\": [\n",
        "            time_ngcf_yelp,\n",
        "            time_lightgcn_yelp,\n",
        "            time_ngcf_amazon,\n",
        "            time_lightgcn_amazon,\n",
        "            time_ultragcn_yelp,\n",
        "            time_ultragcn_amazon,\n",
        "        ],\n",
        "    }\n",
        "\n",
        "    results_df = pd.DataFrame(results_data)\n",
        "    print(\"\\nüìä TABELA DE RESULTADOS FINAIS:\")\n",
        "    print(results_df.round(4))\n",
        "\n",
        "    # An√°lise comparativa\n",
        "    print(\"\\nüìà AN√ÅLISE COMPARATIVA:\")\n",
        "    print(\"\\n1. NGCF vs LightGCN:\")\n",
        "    yelp_ngcf_vs_light = recall_lightgcn_yelp - recall_ngcf_yelp\n",
        "    amazon_ngcf_vs_light = recall_lightgcn_amazon - recall_ngcf_amazon\n",
        "    print(f\"   ‚Ä¢ Yelp: LightGCN supera NGCF em {yelp_ngcf_vs_light:+.4f} (Recall@20)\")\n",
        "    print(\n",
        "        f\"   ‚Ä¢ Amazon: LightGCN supera NGCF em {amazon_ngcf_vs_light:+.4f} (Recall@20)\"\n",
        "    )\n",
        "\n",
        "    print(\"\\n2. Melhor modelo por dataset:\")\n",
        "    if recall_ultragcn_yelp > max(recall_ngcf_yelp, recall_lightgcn_yelp):\n",
        "        print(f\"   ‚Ä¢ Yelp: UltraGCN ({recall_ultragcn_yelp:.4f})\")\n",
        "    else:\n",
        "        best_yelp = \"LightGCN\" if recall_lightgcn_yelp > recall_ngcf_yelp else \"NGCF\"\n",
        "        best_yelp_score = max(recall_lightgcn_yelp, recall_ngcf_yelp)\n",
        "        print(f\"   ‚Ä¢ Yelp: {best_yelp} ({best_yelp_score:.4f})\")\n",
        "\n",
        "    if recall_ultragcn_amazon > max(recall_ngcf_amazon, recall_lightgcn_amazon):\n",
        "        print(f\"   ‚Ä¢ Amazon: UltraGCN ({recall_ultragcn_amazon:.4f})\")\n",
        "    else:\n",
        "        best_amazon = (\n",
        "            \"LightGCN\" if recall_lightgcn_amazon > recall_ngcf_amazon else \"NGCF\"\n",
        "        )\n",
        "        best_amazon_score = max(recall_lightgcn_amazon, recall_ngcf_amazon)\n",
        "        print(f\"   ‚Ä¢ Amazon: {best_amazon} ({best_amazon_score:.4f})\")\n",
        "\n",
        "    print(\"\\n3. Tempo de treinamento:\")\n",
        "    avg_time_ngcf = (time_ngcf_yelp + time_ngcf_amazon) / 2\n",
        "    avg_time_lightgcn = (time_lightgcn_yelp + time_lightgcn_amazon) / 2\n",
        "    avg_time_ultragcn = (time_ultragcn_yelp + time_ultragcn_amazon) / 2\n",
        "\n",
        "    print(f\"   ‚Ä¢ NGCF: {avg_time_ngcf:.2f}s (m√©dia)\")\n",
        "    print(f\"   ‚Ä¢ LightGCN: {avg_time_lightgcn:.2f}s (m√©dia)\")\n",
        "    print(f\"   ‚Ä¢ UltraGCN: {avg_time_ultragcn:.2f}s (m√©dia)\")\n",
        "\n",
        "    print(\"\\n‚úÖ EXPERIMENTO CONCLU√çDO!\")\n",
        "    print(\"=\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4361d41",
      "metadata": {
        "id": "e4361d41"
      },
      "source": [
        "## üéØ Conclus√µes e Pr√≥ximos Passos\n",
        "\n",
        "### üìä O que foi implementado:\n",
        "\n",
        "- ‚úÖ **3 arquiteturas GNN**: NGCF, LightGCN e UltraGCN\n",
        "- ‚úÖ **2 datasets**: Yelp e Amazon Books com pr√©-processamento\n",
        "- ‚úÖ **M√©tricas robustas**: Recall@20 e NDCG@20\n",
        "- ‚úÖ **Otimiza√ß√µes**: Batching, valida√ß√µes e configura√ß√µes para Kaggle\n",
        "\n",
        "### üî¨ Resultados esperados:\n",
        "\n",
        "- **LightGCN** deve superar **NGCF** em efici√™ncia e simplicidade\n",
        "- **UltraGCN** deve apresentar os melhores resultados de precis√£o\n",
        "- Datasets maiores (Amazon) podem favorecer arquiteturas mais complexas\n",
        "\n",
        "### üöÄ Pr√≥ximos passos:\n",
        "\n",
        "1. Experimentos com hiperpar√¢metros diferentes\n",
        "2. Testes em datasets maiores\n",
        "3. Implementa√ß√£o de outras arquiteturas (LightGCL, NGCL)\n",
        "4. An√°lise mais profunda dos embeddings aprendidos\n",
        "\n",
        "### üìù Para usar este notebook:\n",
        "\n",
        "- **Kaggle**: Upload direto e execute\n",
        "- **Local**: Siga as instru√ß√µes no README.md\n",
        "- **Customiza√ß√£o**: Ajuste hiperpar√¢metros na se√ß√£o correspondente\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aed4b036",
      "metadata": {
        "id": "aed4b036"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# LIMPEZA FINAL E INFORMA√á√ïES DO SISTEMA\n",
        "# =============================================================================\n",
        "\n",
        "# Tentar importar psutil, se n√£o estiver dispon√≠vel, usar alternativa\n",
        "try:\n",
        "    import psutil\n",
        "\n",
        "    psutil_available = True\n",
        "except ImportError:\n",
        "    psutil_available = False\n",
        "    print(\"‚ö†Ô∏è  psutil n√£o dispon√≠vel - algumas informa√ß√µes de sistema ser√£o limitadas\")\n",
        "\n",
        "print(\"=== LIMPEZA DE MEM√ìRIA ===\")\n",
        "\n",
        "# Limpeza de mem√≥ria\n",
        "if \"model_ngcf_yelp\" in locals():\n",
        "    del model_ngcf_yelp\n",
        "if \"model_lightgcn_yelp\" in locals():\n",
        "    del model_lightgcn_yelp\n",
        "if \"model_ultragcn_yelp\" in locals():\n",
        "    del model_ultragcn_yelp\n",
        "if \"model_ngcf_amazon\" in locals():\n",
        "    del model_ngcf_amazon\n",
        "if \"model_lightgcn_amazon\" in locals():\n",
        "    del model_lightgcn_amazon\n",
        "if \"model_ultragcn_amazon\" in locals():\n",
        "    del model_ultragcn_amazon\n",
        "\n",
        "# Limpar cache do PyTorch\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"‚úì Cache GPU limpo\")\n",
        "\n",
        "# Garbage collection\n",
        "gc.collect()\n",
        "print(\"‚úì Garbage collection executado\")\n",
        "\n",
        "# Informa√ß√µes do sistema\n",
        "print(\"\\n=== INFORMA√á√ïES DO SISTEMA ===\")\n",
        "print(f\"Vers√£o Python: {sys.version.split()[0]}\")\n",
        "print(f\"Vers√£o PyTorch: {torch.__version__}\")\n",
        "print(f\"Vers√£o PyTorch Geometric: {torch_geometric.__version__}\")\n",
        "print(f\"Dispositivo usado: {device}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(\n",
        "        f\"Mem√≥ria GPU total: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\"\n",
        "    )\n",
        "    print(\n",
        "        f\"Mem√≥ria GPU livre: {(torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated()) / 1e9:.1f} GB\"\n",
        "    )\n",
        "\n",
        "# Informa√ß√µes de mem√≥ria RAM\n",
        "if psutil_available:\n",
        "    memory_info = psutil.virtual_memory()\n",
        "    print(f\"RAM total: {memory_info.total / 1e9:.1f} GB\")\n",
        "    print(f\"RAM dispon√≠vel: {memory_info.available / 1e9:.1f} GB\")\n",
        "    print(f\"RAM em uso: {memory_info.percent:.1f}%\")\n",
        "else:\n",
        "    print(\"RAM: Informa√ß√µes n√£o dispon√≠veis (psutil n√£o instalado)\")\n",
        "\n",
        "print(\"\\n‚úÖ Experimento finalizado com sucesso!\")\n",
        "print(\"üìÅ Arquivos criados: README.md, environment.yml, requirements.txt, .gitignore\")\n",
        "print(\"üéâ Pronto para compartilhar ou continuar desenvolvimento!\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"üöÄ OBRIGADO POR USAR O GNN RECOMMENDER SYSTEM! üöÄ\")\n",
        "print(\"=\" * 60)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "nlp_hate",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.23"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}